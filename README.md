# VQA-Project

Extracting text from an image with a video Q&A system has become an important area of the combination of computer vision and natural language processing and has so far been able to attract a lot of attention. The method of extracting text from an image with an intuitive question and answer system with the help of deep learning and neural networks has made tremendous progress in helping blind people, for example to read scene texts or to drive a car automatically. In our research, we have tried to improve the accuracy of the Scene TextVQA model by using the YOLO Object Recognition Network. The system will also be able to detect and identify text in natural scenes using the OCR module. OCR is able to extract text from clean documents; Therefore, understanding the text in natural scenes due to curvature, distortion, background, different fonts and other challenges has a much higher level of OCR and will lead to the use of deep learning. 


In the following, we will see an example of this system:

![image](https://user-images.githubusercontent.com/83070820/174563763-6ac6610b-6d1f-4ab7-a999-725686912624.png)

